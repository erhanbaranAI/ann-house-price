{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1588da7f-a5db-4dfb-b5ee-cf1721065870",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install tensorflow\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebaa0714-5c70-486b-a37a-38ba5694ee9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "def preprocess_dataset_train(file_path):\n",
    "    \"\"\"\n",
    "    Train veri setini hazırlar: Eksik değerleri doldurur, kategorik değişkenleri işler,\n",
    "    log dönüşümü uygular, düşük korelasyonlu sütunları temizler ve IQR yöntemiyle aykırı\n",
    "    değerleri sınırlar.\n",
    "    \"\"\"\n",
    "    # 📌 1. CSV dosyasını oku\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"✅ Veri yüklendi: {df.shape}\")\n",
    "\n",
    "    # 🟢 2. Log dönüşümünü uygula (ÖNEMLİ!)\n",
    "    df[\"SalePrice\"] = np.log1p(df[\"SalePrice\"])\n",
    "    print(\"✅ Log dönüşümü uygulandı!\")\n",
    "\n",
    "    # 🟢 3. Eksik değerleri doldur\n",
    "    df[\"GarageYrBlt\"].fillna(0, inplace=True)\n",
    "    df[\"LotFrontage\"] = df.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))\n",
    "    df[\"MasVnrArea\"].fillna(0, inplace=True)\n",
    "    print(\"✅ Eksik değerler dolduruldu!\")\n",
    "\n",
    "    # 🟢 4. Ordinal Encoding Uygula\n",
    "    ordinal_features = {\n",
    "        \"ExterQual\": {\"Ex\": 5, \"Gd\": 4, \"TA\": 3, \"Fa\": 2, \"Po\": 1},\n",
    "        \"ExterCond\": {\"Ex\": 5, \"Gd\": 4, \"TA\": 3, \"Fa\": 2, \"Po\": 1},\n",
    "        \"BsmtQual\": {\"Ex\": 5, \"Gd\": 4, \"TA\": 3, \"Fa\": 2, \"Po\": 1, \"NA\": 0},\n",
    "        \"BsmtCond\": {\"Ex\": 5, \"Gd\": 4, \"TA\": 3, \"Fa\": 2, \"Po\": 1, \"NA\": 0},\n",
    "        \"KitchenQual\": {\"Ex\": 5, \"Gd\": 4, \"TA\": 3, \"Fa\": 2, \"Po\": 1},\n",
    "        \"FireplaceQu\": {\"Ex\": 5, \"Gd\": 4, \"TA\": 3, \"Fa\": 2, \"Po\": 1, \"NA\": 0},\n",
    "        \"GarageQual\": {\"Ex\": 5, \"Gd\": 4, \"TA\": 3, \"Fa\": 2, \"Po\": 1, \"NA\": 0},\n",
    "        \"GarageCond\": {\"Ex\": 5, \"Gd\": 4, \"TA\": 3, \"Fa\": 2, \"Po\": 1, \"NA\": 0},\n",
    "        \"PoolQC\": {\"Ex\": 5, \"Gd\": 4, \"TA\": 3, \"Fa\": 2, \"NA\": 0},\n",
    "        \"Fence\": {\"GdPrv\": 4, \"MnPrv\": 3, \"GdWo\": 2, \"MnWw\": 1, \"NA\": 0}\n",
    "    }\n",
    "    \n",
    "    for col, mapping in ordinal_features.items():\n",
    "        df[col] = df[col].map(mapping)\n",
    "\n",
    "    print(\"✅ Ordinal Encoding tamamlandı!\")\n",
    "\n",
    "    # 🟢 5. One-Hot Encoding\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "    df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n",
    "    print(\"✅ One-Hot Encoding tamamlandı!\")\n",
    "\n",
    "    # 🟢 6. SalePrice ile düşük korelasyonlu sütunları kaldır\n",
    "    correlation_threshold = 0.05\n",
    "    corr_with_saleprice = df.corr()[\"SalePrice\"].abs()\n",
    "    low_corr_features = corr_with_saleprice[corr_with_saleprice < correlation_threshold].index\n",
    "    df.drop(columns=low_corr_features, inplace=True)\n",
    "    print(f\"✅ Düşük korelasyonlu {len(low_corr_features)} sütun kaldırıldı!\")\n",
    "\n",
    "    # 🟢 7. Sayısal değişkenlerdeki eksik değerleri MEDIAN ile doldur\n",
    "    numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    missing_before = df[numerical_cols].isna().sum().sum()\n",
    "    df[numerical_cols] = df[numerical_cols].apply(lambda x: x.fillna(x.median()))\n",
    "    print(f\"✅ Sayısal eksik değerler median ile dolduruldu! Toplam değiştirilen hücre sayısı: {missing_before}\")\n",
    "\n",
    "    # 🟢 8. Aykırı Değerleri IQR ile Kırp\n",
    "    def remove_outliers_iqr(df, columns):\n",
    "        for col in columns:\n",
    "            Q1 = df[col].quantile(0.25)\n",
    "            Q3 = df[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            df[col] = np.clip(df[col], lower_bound, upper_bound)\n",
    "        return df\n",
    "\n",
    "    df = remove_outliers_iqr(df, numerical_cols)\n",
    "    print(\"✅ Aykırı değerler IQR yöntemiyle kırpıldı!\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc03d7ea-ee6d-4834-9804-991aa6c2cbba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Veri yüklendi: (1460, 81)\n",
      "✅ Log dönüşümü uygulandı!\n",
      "✅ Eksik değerler dolduruldu!\n",
      "✅ Ordinal Encoding tamamlandı!\n",
      "✅ One-Hot Encoding tamamlandı!\n",
      "✅ Düşük korelasyonlu 76 sütun kaldırıldı!\n",
      "✅ Sayısal eksik değerler median ile dolduruldu! Toplam değiştirilen hücre sayısı: 3558\n",
      "✅ Aykırı değerler IQR yöntemiyle kırpıldı!\n"
     ]
    }
   ],
   "source": [
    "# Train veri setini işle\n",
    "train_prepared = preprocess_dataset_train(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88213caa-2425-4c6b-9a12-6f8dceb527e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def train_ann(train_df):\n",
    "    \"\"\"\n",
    "    Daha derin ve yavaş öğrenen Yapay Sinir Ağı (ANN) modelini eğitir.\n",
    "    \"\"\"\n",
    "    y = train_df[\"SalePrice\"]\n",
    "    X = train_df.drop(columns=[\"SalePrice\"])\n",
    "\n",
    "    # Train-Test Ayırma\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "    # Modeli oluştur\n",
    "    model = Sequential([\n",
    "        Dense(256, activation=\"relu\", input_shape=(X_train.shape[1],)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        Dense(128, activation=\"relu\"),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        Dense(64, activation=\"relu\"),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        Dense(32, activation=\"relu\"),\n",
    "        Dense(16, activation=\"relu\"),\n",
    "        Dense(1, activation=\"linear\")  # Regresyon için \"linear\" aktivasyon\n",
    "    ])\n",
    "\n",
    "    # Modeli Derleme\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0001), loss=\"mse\", metrics=[\"mae\"])\n",
    "\n",
    "    # Early Stopping Callback (Eğer gelişme durursa eğitimi erken bitirir)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "\n",
    "    # Modeli Eğitme\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_test, y_test), \n",
    "                        epochs=200, batch_size=16, callbacks=[early_stopping], verbose=1)\n",
    "\n",
    "    # Modeli Kaydetme\n",
    "    model.save(\"ann_model.h5\")\n",
    "    print(\"✅ Yapay Sinir Ağı Modeli Eğitildi!\")\n",
    "\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "441742a4-eff4-4df0-b0fd-a217463127fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 155.1963 - mae: 12.4262 - val_loss: 151.4172 - val_mae: 12.2840\n",
      "Epoch 2/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 141.4251 - mae: 11.8558 - val_loss: 126.8328 - val_mae: 11.2169\n",
      "Epoch 3/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 124.2652 - mae: 11.0856 - val_loss: 110.8270 - val_mae: 10.4147\n",
      "Epoch 4/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 108.1396 - mae: 10.2831 - val_loss: 91.3768 - val_mae: 9.3669\n",
      "Epoch 5/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 90.3751 - mae: 9.3144 - val_loss: 72.6985 - val_mae: 8.2158\n",
      "Epoch 6/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 72.3374 - mae: 8.2075 - val_loss: 53.5626 - val_mae: 6.9119\n",
      "Epoch 7/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 53.1867 - mae: 6.8489 - val_loss: 37.7033 - val_mae: 5.6429\n",
      "Epoch 8/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 39.8310 - mae: 5.7222 - val_loss: 25.9804 - val_mae: 4.4978\n",
      "Epoch 9/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 25.7042 - mae: 4.4262 - val_loss: 17.8984 - val_mae: 3.5091\n",
      "Epoch 10/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 18.7580 - mae: 3.6631 - val_loss: 11.3555 - val_mae: 2.6912\n",
      "Epoch 11/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 12.9151 - mae: 2.9758 - val_loss: 7.4777 - val_mae: 2.0429\n",
      "Epoch 12/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 9.2252 - mae: 2.4886 - val_loss: 4.9782 - val_mae: 1.5774\n",
      "Epoch 13/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 7.4908 - mae: 2.1840 - val_loss: 3.4719 - val_mae: 1.2304\n",
      "Epoch 14/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5.8192 - mae: 1.9664 - val_loss: 2.5656 - val_mae: 0.9703\n",
      "Epoch 15/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5.4013 - mae: 1.8816 - val_loss: 1.9294 - val_mae: 0.8575\n",
      "Epoch 16/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.4487 - mae: 1.6835 - val_loss: 1.5564 - val_mae: 0.8456\n",
      "Epoch 17/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.5842 - mae: 1.7171 - val_loss: 1.5533 - val_mae: 0.8757\n",
      "Epoch 18/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.4565 - mae: 1.6862 - val_loss: 1.5528 - val_mae: 0.9247\n",
      "Epoch 19/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.6133 - mae: 1.7381 - val_loss: 1.3587 - val_mae: 0.8527\n",
      "Epoch 20/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.0246 - mae: 1.6176 - val_loss: 1.1738 - val_mae: 0.8107\n",
      "Epoch 21/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.5649 - mae: 1.5116 - val_loss: 1.0548 - val_mae: 0.8225\n",
      "Epoch 22/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.0500 - mae: 1.6078 - val_loss: 1.0196 - val_mae: 0.7710\n",
      "Epoch 23/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.6631 - mae: 1.5354 - val_loss: 1.0580 - val_mae: 0.8099\n",
      "Epoch 24/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.9275 - mae: 1.6092 - val_loss: 0.9136 - val_mae: 0.7625\n",
      "Epoch 25/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.6671 - mae: 1.5641 - val_loss: 0.9254 - val_mae: 0.7518\n",
      "Epoch 26/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.1992 - mae: 1.6386 - val_loss: 0.9246 - val_mae: 0.7545\n",
      "Epoch 27/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.9091 - mae: 1.6017 - val_loss: 0.8482 - val_mae: 0.7162\n",
      "Epoch 28/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.4476 - mae: 1.5021 - val_loss: 0.7334 - val_mae: 0.6493\n",
      "Epoch 29/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.4467 - mae: 1.4978 - val_loss: 0.8793 - val_mae: 0.7451\n",
      "Epoch 30/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.1880 - mae: 1.4385 - val_loss: 0.9170 - val_mae: 0.7482\n",
      "Epoch 31/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.4074 - mae: 1.4979 - val_loss: 0.8570 - val_mae: 0.7044\n",
      "Epoch 32/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.3716 - mae: 1.4846 - val_loss: 0.9520 - val_mae: 0.7454\n",
      "Epoch 33/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.9403 - mae: 1.3879 - val_loss: 0.8970 - val_mae: 0.7091\n",
      "Epoch 34/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.1179 - mae: 1.3950 - val_loss: 0.9079 - val_mae: 0.7085\n",
      "Epoch 35/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.8557 - mae: 1.3455 - val_loss: 0.9222 - val_mae: 0.7178\n",
      "Epoch 36/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.1800 - mae: 1.4288 - val_loss: 1.0883 - val_mae: 0.7887\n",
      "Epoch 37/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.8083 - mae: 1.3488 - val_loss: 1.0654 - val_mae: 0.7868\n",
      "Epoch 38/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.1884 - mae: 1.4139 - val_loss: 0.8993 - val_mae: 0.7078\n",
      "Epoch 39/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.9418 - mae: 1.3731 - val_loss: 0.9859 - val_mae: 0.7249\n",
      "Epoch 40/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.6322 - mae: 1.3179 - val_loss: 0.9679 - val_mae: 0.7101\n",
      "Epoch 41/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.9187 - mae: 1.3580 - val_loss: 1.1155 - val_mae: 0.7856\n",
      "Epoch 42/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.6130 - mae: 1.3016 - val_loss: 1.1166 - val_mae: 0.7602\n",
      "Epoch 43/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.4734 - mae: 1.2675 - val_loss: 1.1190 - val_mae: 0.7411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Yapay Sinir Ağı Modeli Eğitildi!\n"
     ]
    }
   ],
   "source": [
    "# Modeli eğit ve çıktıları al\n",
    "train_ann_model = train_ann(train_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5503dcb3-d0bf-4f06-95cf-1a5accf1db79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def preprocess_dataset_test(file_path, train_columns):\n",
    "    \"\"\"\n",
    "    Test veri setini işler ve train setindeki sütunlarla uyumlu hale getirir.\n",
    "    - Eksik değerleri doldurur.\n",
    "    - Kategorik değişkenleri işler.\n",
    "    - One-Hot Encoding uygular.\n",
    "    - Eksik sütunları sıfır ile doldurur.\n",
    "    \"\"\"\n",
    "    # 📌 1. CSV dosyasını oku\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"✅ Test verisi yüklendi: {df.shape}\")\n",
    "\n",
    "    # 🟢 2. GarageYrBlt, LotFrontage ve MasVnrArea eksik değerlerini doldur\n",
    "    df[\"GarageYrBlt\"].fillna(0, inplace=True)\n",
    "    df[\"LotFrontage\"] = df.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))\n",
    "    df[\"MasVnrArea\"].fillna(0, inplace=True)\n",
    "\n",
    "    print(\"✅ Eksik değerler dolduruldu!\")\n",
    "\n",
    "    # 🟢 3. Ordinal Encoding Uygula\n",
    "    ordinal_features = {\n",
    "        \"ExterQual\": {\"Ex\": 5, \"Gd\": 4, \"TA\": 3, \"Fa\": 2, \"Po\": 1},\n",
    "        \"ExterCond\": {\"Ex\": 5, \"Gd\": 4, \"TA\": 3, \"Fa\": 2, \"Po\": 1},\n",
    "        \"BsmtQual\": {\"Ex\": 5, \"Gd\": 4, \"TA\": 3, \"Fa\": 2, \"Po\": 1, \"NA\": 0},\n",
    "        \"BsmtCond\": {\"Ex\": 5, \"Gd\": 4, \"TA\": 3, \"Fa\": 2, \"Po\": 1, \"NA\": 0},\n",
    "        \"KitchenQual\": {\"Ex\": 5, \"Gd\": 4, \"TA\": 3, \"Fa\": 2, \"Po\": 1},\n",
    "        \"FireplaceQu\": {\"Ex\": 5, \"Gd\": 4, \"TA\": 3, \"Fa\": 2, \"Po\": 1, \"NA\": 0},\n",
    "        \"GarageQual\": {\"Ex\": 5, \"Gd\": 4, \"TA\": 3, \"Fa\": 2, \"Po\": 1, \"NA\": 0},\n",
    "        \"GarageCond\": {\"Ex\": 5, \"Gd\": 4, \"TA\": 3, \"Fa\": 2, \"Po\": 1, \"NA\": 0},\n",
    "        \"PoolQC\": {\"Ex\": 5, \"Gd\": 4, \"TA\": 3, \"Fa\": 2, \"NA\": 0},\n",
    "        \"Fence\": {\"GdPrv\": 4, \"MnPrv\": 3, \"GdWo\": 2, \"MnWw\": 1, \"NA\": 0}\n",
    "    }\n",
    "    \n",
    "    for col, mapping in ordinal_features.items():\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].map(mapping)\n",
    "\n",
    "    print(\"✅ Ordinal Encoding tamamlandı!\")\n",
    "\n",
    "    # 🟢 4. One-Hot Encoding Uygula\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "    df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n",
    "    print(\"✅ One-Hot Encoding tamamlandı!\")\n",
    "\n",
    "    # 🟢 5. Train veri setindeki sütunlarla test verisini uyumlu hale getir\n",
    "    missing_cols = set(train_columns) - set(df.columns)\n",
    "    for col in missing_cols:\n",
    "        df[col] = 0  # Eksik sütunları sıfır ile doldur\n",
    "    \n",
    "    df = df[train_columns]  # Fazla olan sütunları kaldır\n",
    "\n",
    "    print(f\"✅ Test seti, train setiyle uyumlu hale getirildi! Yeni şekil: {df.shape}\")\n",
    "\n",
    "    # 🟢 6. Eksik Sayısal Değerleri Median ile Doldur\n",
    "    numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    missing_before = df[numerical_cols].isna().sum().sum()\n",
    "    df[numerical_cols] = df[numerical_cols].apply(lambda x: x.fillna(x.median()))\n",
    "    print(f\"✅ Sayısal eksik değerler median ile dolduruldu! Toplam değiştirilen hücre sayısı: {missing_before}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4217d5e-357c-44aa-93f1-2b54b63278d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def test_ann(model, test_df, test_csv_path, output_csv=\"ann-submission.csv\"):\n",
    "    \"\"\"\n",
    "    Eğitilmiş ANN modelini test verisi ile kullanarak tahmin yapar.\n",
    "    Log dönüşümünü ters çevirerek gerçek değerleri verir.\n",
    "    \"\"\"\n",
    "    # Test verisini yükle\n",
    "    test_raw = pd.read_csv(test_csv_path)\n",
    "    test_ids = test_raw[\"Id\"]  # Orijinal test setindeki ID'leri al\n",
    "\n",
    "    # Model ile tahmin yap\n",
    "    predictions_log = model.predict(test_df).flatten()\n",
    "\n",
    "    # 📌 Log dönüşümünü geri al\n",
    "    predictions = np.expm1(predictions_log)  # 🔥 EKLENDİ\n",
    "\n",
    "    # Tahminleri kaydet\n",
    "    submission = pd.DataFrame({\"Id\": test_ids, \"SalePrice\": predictions})\n",
    "    submission.to_csv(output_csv, index=False)\n",
    "    print(f\"✅ Tahminler {output_csv} dosyasına kaydedildi!\")\n",
    "\n",
    "# 📌 Test verisini hazırla ve modeli test et\n",
    "test_prepared = preprocess_dataset_test(\"test.csv\", train_prepared.drop(columns=[\"SalePrice\"]).columns)\n",
    "test_ann(train_ann_model, test_prepared, \"test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97d8e15e-b6a3-4081-b453-08981ff314dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Test verisi yüklendi: (1459, 80)\n",
      "✅ Eksik değerler dolduruldu!\n",
      "✅ Ordinal Encoding tamamlandı!\n",
      "✅ One-Hot Encoding tamamlandı!\n",
      "✅ Test seti, train setiyle uyumlu hale getirildi! Yeni şekil: (1459, 146)\n",
      "✅ Sayısal eksik değerler median ile dolduruldu! Toplam değiştirilen hücre sayısı: 3608\n"
     ]
    }
   ],
   "source": [
    "# 📌 1. Test veri setini işle (train veri setiyle uyumlu hale getir)\n",
    "test_prepared = preprocess_dataset_test(\"test.csv\", train_prepared.drop(columns=[\"SalePrice\"]).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f07f5cd-eb42-47a5-83fe-5a6dd122959a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "✅ Tahminler ann-submission.csv dosyasına kaydedildi!\n"
     ]
    }
   ],
   "source": [
    "# 📌 2. Modeli kullanarak tahmin yap ve submission.csv dosyasını oluştur\n",
    "test_ann(train_ann_model, test_prepared, \"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d4f3d8-8a5d-4c80-b786-eb29abdab3cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
